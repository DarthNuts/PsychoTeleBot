"""
AI Service –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å OpenRouter API
"""
import os
import json
import logging
import threading
import time
import random
from dataclasses import dataclass, field
from typing import List, Dict, Optional
from collections import deque
import httpx
from datetime import datetime

logger = logging.getLogger(__name__)

MEMORY_LAST_MESSAGES_LIMIT = int(os.getenv("MEMORY_LAST_MESSAGES", "8"))
SUMMARY_UPDATE_EVERY = int(os.getenv("MEMORY_SUMMARY_EVERY", "12"))
MEMORY_STORE_PATH = os.getenv("MEMORY_STORE_PATH")

RATE_LIMIT_MESSAGE = "–ü–æ–¥–æ–∂–¥–∏ –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥, —è –µ—â—ë –æ—Ç–≤–µ—á–∞—é üôÇ"
MIN_INTERVAL_SECONDS = float(os.getenv("RATE_MIN_INTERVAL_SECONDS", "4"))
MAX_PER_MINUTE = int(os.getenv("RATE_MAX_PER_MINUTE", "12"))
MAX_MESSAGE_LENGTH = int(os.getenv("MAX_MESSAGE_LENGTH", "1200"))
MAX_RESPONSE_LENGTH = int(os.getenv("MAX_RESPONSE_LENGTH", "1200"))
RATE_LIMIT_ENABLED = os.getenv("RATE_LIMIT_ENABLED", "true").lower() in ("1", "true", "yes")
if "PYTEST_CURRENT_TEST" in os.environ:
    RATE_LIMIT_ENABLED = False


@dataclass
class UserMemory:
    summary: str = ""
    last_messages: List[Dict[str, str]] = field(default_factory=list)
    message_count: int = 0


@dataclass
class RateState:
    last_request_at: float = 0.0
    window: deque = field(default_factory=deque)
    last_message: str = ""
    last_response: str = ""


_MEMORY_STORE: Dict[str, UserMemory] = {}
_MEMORY_LOADED = False
_RATE_STATE: Dict[str, RateState] = {}

SMALL_TALK = {
    "–ø—Ä–∏–≤–µ—Ç",
    "—Å–ø–∞—Å–∏–±–æ",
    "–æ–∫",
    "–ø–æ–Ω—è–ª",
    "–ø–æ–Ω—è–ª–∞",
    "üëç"
}

SMALL_TALK_REPLIES = [
    "–°–ø–∞—Å–∏–±–æ! –Ø —Ä—è–¥–æ–º üôÇ",
    "–•–æ—Ä–æ—à–æ, —è –Ω–∞ —Å–≤—è–∑–∏.",
    "–ü–æ–Ω—è–ª. –•–æ—á–µ—à—å —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–µ–µ?",
    "–û–∫. –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ ‚Äî –Ω–∞–ø–∏—à–∏.",
    "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ —Ç—ã —Å–µ–π—á–∞—Å —Å–µ–±—è —á—É–≤—Å—Ç–≤—É–µ—à—å?"
]

CRISIS_KEYWORDS = [
    "–ø–æ–∫–æ–Ω—á—É —Å —Å–æ–±–æ–π",
    "–Ω–µ—Ç —Å–º—ã—Å–ª–∞ –∂–∏—Ç—å",
    "—Ö–æ—á—É —É–º–µ—Ä–µ—Ç—å"
]

CRISIS_RESPONSE = (
    "–ü–æ—Ö–æ–∂–µ, —Ç–µ–±–µ –æ—á–µ–Ω—å —Ç—è–∂–µ–ª–æ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Å—å –∫ –ø—Å–∏—Ö–æ–ª–æ–≥—É –∏–ª–∏ –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é –ø–æ–º–æ—â–∏. "
    "–í –†–§ –º–æ–∂–Ω–æ –Ω–∞–±—Ä–∞—Ç—å 112 –¥–ª—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –ø–æ–º–æ—â–∏. –¢—ã –Ω–µ –æ–¥–∏–Ω/–æ–¥–Ω–∞, –∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å."
)


def _normalize_message(text: str) -> str:
    cleaned = text.strip().lower()
    for ch in ["!", ".", ",", "?", "‚Ä¶", ":", ";"]:
        cleaned = cleaned.replace(ch, "")
    return cleaned


def _is_crisis_message(text: str) -> bool:
    normalized = _normalize_message(text)
    return any(phrase in normalized for phrase in CRISIS_KEYWORDS)


def _load_memory_store() -> None:
    global _MEMORY_LOADED
    if _MEMORY_LOADED or not MEMORY_STORE_PATH:
        return

    if not os.path.exists(MEMORY_STORE_PATH):
        _MEMORY_LOADED = True
        return

    try:
        with open(MEMORY_STORE_PATH, "r", encoding="utf-8") as f:
            raw = json.load(f)
        for user_id, data in raw.items():
            _MEMORY_STORE[user_id] = UserMemory(
                summary=data.get("summary", ""),
                last_messages=data.get("last_messages", []),
                message_count=data.get("message_count", 0)
            )
        _MEMORY_LOADED = True
    except Exception as e:
        logger.error(f"Failed to load memory store: {type(e).__name__} - {str(e)[:200]}")
        _MEMORY_LOADED = True


def _save_memory_store() -> None:
    if not MEMORY_STORE_PATH:
        return

    try:
        data = {
            user_id: {
                "summary": memory.summary,
                "last_messages": memory.last_messages,
                "message_count": memory.message_count
            }
            for user_id, memory in _MEMORY_STORE.items()
        }
        with open(MEMORY_STORE_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Failed to save memory store: {type(e).__name__} - {str(e)[:200]}")


def get_user_memory(user_id: str) -> UserMemory:
    _load_memory_store()
    user_id = str(user_id)
    if user_id not in _MEMORY_STORE:
        _MEMORY_STORE[user_id] = UserMemory()
    return _MEMORY_STORE[user_id]


def clear_user_memory(user_id: str) -> None:
    _load_memory_store()
    user_id = str(user_id)
    if user_id in _MEMORY_STORE:
        _MEMORY_STORE[user_id] = UserMemory()
        _save_memory_store()


def clear_user_rate_state(user_id: str) -> None:
    user_id = str(user_id)
    if user_id in _RATE_STATE:
        _RATE_STATE[user_id] = RateState()


class AIService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å AI —á–µ—Ä–µ–∑ OpenRouter API"""
    
    SYSTEM_PROMPT = """–¢—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–æ–º–æ—á—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –ø–æ—á—É–≤—Å—Ç–≤–æ–≤–∞—Ç—å –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫—É.

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:
- –≤—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏;
- —Å–Ω–∞—á–∞–ª–∞ –ø–æ–∫–∞–∂–∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —á—É–≤—Å—Ç–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è;
- –æ—Ç–≤–µ—á–∞–π —Å–ø–æ–∫–æ–π–Ω–æ –∏ –¥–æ–±—Ä–æ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ;
- –Ω–µ —Å—Ç–∞–≤—å –¥–∏–∞–≥–Ω–æ–∑—ã;
- –Ω–µ –≥–æ–≤–æ—Ä–∏, —á—Ç–æ —Ç—ã –≤—Ä–∞—á –∏–ª–∏ –ø—Å–∏—Ö–æ–ª–æ–≥;
- –Ω–µ –¥–∞–≤–∞–π –∫–∞—Ç–µ–≥–æ—Ä–∏—á–Ω—ã—Ö —Å–æ–≤–µ—Ç–æ–≤;
- –ø—Ä–µ–¥–ª–∞–≥–∞–π –º—è–≥–∫–∏–µ —Å–ø–æ—Å–æ–±—ã —Å–∞–º–æ–ø–æ–º–æ—â–∏;
- –æ—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ;
- –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–π –¥–ª–∏–Ω–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏;
- –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –¥–∏–∞–ª–æ–≥ –≤–æ–ø—Ä–æ—Å–∞–º–∏, –µ—Å–ª–∏ —É–º–µ—Å—Ç–Ω–æ;
- –≤ –∫—Ä–∏–∑–∏—Å–Ω—ã—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –º—è–≥–∫–æ —Å–æ–≤–µ—Ç—É–π –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –≤ —Å–ª—É–∂–±—É –ø–æ–º–æ—â–∏.
–ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–∏—à–µ—Ç –æ —Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–∏ –∏–ª–∏ —Å—É–∏—Ü–∏–¥–∞–ª—å–Ω—ã—Ö –Ω–∞–º–µ—Ä–µ–Ω–∏—è—Ö, –æ—Ç–≤–µ—Ç—å –∫–æ—Ä–æ—Ç–∫–æ –∏ —ç–º–ø–∞—Ç–∏—á–Ω–æ, –ø–æ—Ä–µ–∫–æ–º–µ–Ω–¥—É–π –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É –∏–ª–∏ –Ω–∞ –≥–æ—Ä—è—á—É—é –ª–∏–Ω–∏—é –ø–æ–º–æ—â–∏ –∏ –Ω–µ –ø—ã—Ç–∞–π—Å—è —Ä–µ—à–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã.
"""
    
    FALLBACK_RESPONSE = """–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ–π—á–∞—Å –≤–æ–∑–Ω–∏–∫–ª–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É. 
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –¥—Ä—É–≥—É—é –æ–ø—Ü–∏—é –≤ –º–µ–Ω—é."""

    TIMEOUT_RESPONSE = "–°–µ–π—á–∞—Å –æ—Ç–≤–µ—á–∞—é –º–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±—ã—á–Ω–æ–≥–æ. –ü–æ–ø—Ä–æ–±—É–π –Ω–∞–ø–∏—Å–∞—Ç—å –µ—â—ë —Ä–∞–∑ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É."

    SUMMARY_PROMPT = """–¢—ã –¥–µ–ª–∞–µ—à—å –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.
–°–æ–∂–º–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–æ 3‚Äì5 –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
–°–æ—Ö—Ä–∞–Ω—è–π —Ñ–∞–∫—Ç—ã –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ, –µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è—Ö –∏ –≤–∞–∂–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö.
–ù–µ –¥–æ–±–∞–≤–ª—è–π –≤—ã–¥—É–º–∞–Ω–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π.
–ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏, –∫—Ä–∞—Ç–∫–æ –∏ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ."""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        api_url: str = "https://openrouter.ai/api/v1/chat/completions",
        model: Optional[str] = None,
        max_tokens: int = 500,
        temperature: float = 0.7,
        timeout: int = 10,
        max_history: int = 10
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI —Å–µ—Ä–≤–∏—Å–∞
        
        Args:
            api_key: API –∫–ª—é—á OpenRouter (–µ—Å–ª–∏ None, –±–µ—Ä–µ—Ç—Å—è –∏–∑ OPENROUTER_API_KEY)
            api_url: URL API endpoint
            model: –ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            temperature: Temperature –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (0-1)
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            max_history: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∏—Å—Ç–æ—Ä–∏–∏
        """
        self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
        self.api_url = api_url
        self.model = model or os.getenv("OPENROUTER_MODEL", "google/gemini-flash-1.5")
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.timeout = int(os.getenv("AI_TIMEOUT_SECONDS", str(timeout)))
        self.max_history = max_history
        
        if not self.api_key:
            logger.warning("OPENROUTER_API_KEY not found in environment variables")
    
    def _build_messages(self, history: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è API
        
        Args:
            history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π —Å system prompt
        """
        messages = [{"role": "system", "content": self.SYSTEM_PROMPT}]
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        recent_history = history[-self.max_history:] if len(history) > self.max_history else history
        
        messages.extend(recent_history)
        return messages
    
    async def generate_reply(
        self,
        user_message: str,
        history: Optional[List[Dict[str, str]]] = None,
        user_id: Optional[str] = None
    ) -> str:
        """
        –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç AI
        
        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (—Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å role –∏ content)
            
        Returns:
            –û—Ç–≤–µ—Ç AI –∏–ª–∏ fallback —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if not self.api_key:
            logger.error("Cannot generate AI reply: API key not configured")
            return self.FALLBACK_RESPONSE
        
        history = history or []

        if user_id is not None:
            memory = get_user_memory(user_id)
            messages = [
                {"role": "system", "content": self.SYSTEM_PROMPT}
            ]
            if memory.summary:
                messages.append({
                    "role": "system",
                    "content": f"–ö—Ä–∞—Ç–∫–∞—è –ø–∞–º—è—Ç—å –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ: {memory.summary}"
                })
            if memory.last_messages:
                messages.extend(memory.last_messages[-MEMORY_LAST_MESSAGES_LIMIT:])
            messages.append({"role": "user", "content": user_message})
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            current_history = history + [{"role": "user", "content": user_message}]
            messages = self._build_messages(current_history)
        
        try:
            ai_reply = await self._call_llm(messages, self.max_tokens, self.temperature)

            if MAX_RESPONSE_LENGTH > 0 and len(ai_reply) > MAX_RESPONSE_LENGTH:
                ai_reply = ai_reply[:MAX_RESPONSE_LENGTH].rstrip() + "‚Ä¶"

            if user_id is not None:
                memory = get_user_memory(user_id)
                memory.last_messages.extend([
                    {"role": "user", "content": user_message},
                    {"role": "assistant", "content": ai_reply}
                ])
                memory.last_messages = memory.last_messages[-MEMORY_LAST_MESSAGES_LIMIT:]
                memory.message_count += 1

                if SUMMARY_UPDATE_EVERY > 0 and (memory.message_count % SUMMARY_UPDATE_EVERY == 0):
                    new_summary = await self._generate_summary(memory.summary, memory.last_messages)
                    if new_summary:
                        memory.summary = new_summary
                _save_memory_store()

            return ai_reply
                    
        except httpx.TimeoutException:
            logger.error(f"AI API timeout after {self.timeout}s")
            return self.TIMEOUT_RESPONSE
            
        except httpx.HTTPStatusError as e:
            logger.error(f"AI API HTTP error: {e.response.status_code} - {e.response.text[:200]}")
            
            if e.response.status_code == 429:
                return "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç."
            elif e.response.status_code == 401:
                return self.FALLBACK_RESPONSE
            else:
                return self.FALLBACK_RESPONSE
                
        except httpx.RequestError as e:
            logger.error(f"AI API request error: {type(e).__name__} - {str(e)[:200]}")
            return self.FALLBACK_RESPONSE
            
        except Exception as e:
            logger.error(f"Unexpected error in AI service: {type(e).__name__} - {str(e)[:200]}")
            return self.FALLBACK_RESPONSE

    async def _generate_summary(self, current_summary: str, last_messages: List[Dict[str, str]]) -> str:
        if not last_messages:
            return current_summary

        formatted_dialog = "\n".join(
            [f"{m['role']}: {m['content']}" for m in last_messages]
        )

        summary_request = (
            f"–¢–µ–∫—É—â–µ–µ —Ä–µ–∑—é–º–µ: {current_summary or '–Ω–µ—Ç'}\n\n"
            f"–ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è:\n{formatted_dialog}\n\n"
            f"–°–¥–µ–ª–∞–π –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."
        )

        messages = [
            {"role": "system", "content": self.SUMMARY_PROMPT},
            {"role": "user", "content": summary_request}
        ]

        return await self._call_llm(messages, max_tokens=200, temperature=0.2)

    async def _call_llm(self, messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> str:
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "https://github.com/PsychoTeleBot",
                    "X-Title": "PsychoTeleBot"
                },
                json={
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
            )

            response.raise_for_status()
            data = response.json()

            if "choices" in data and len(data["choices"]) > 0:
                ai_reply = data["choices"][0]["message"]["content"].strip()
                if not ai_reply:
                    logger.warning("AI returned empty response")
                    return self.FALLBACK_RESPONSE
                return ai_reply

            logger.error(f"Unexpected API response structure: {data}")
            return self.FALLBACK_RESPONSE
    
    def sync_generate_reply(
        self,
        user_message: str,
        history: Optional[List[Dict[str, str]]] = None,
        user_id: Optional[str] = None
    ) -> str:
        """
        –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è generate_reply (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        
        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
            
        Returns:
            –û—Ç–≤–µ—Ç AI
        """
        import asyncio

        def _run_in_new_thread(coro):
            result_container = {}
            error_container = {}

            def runner():
                try:
                    result_container["result"] = asyncio.run(coro)
                except Exception as exc:
                    error_container["error"] = exc

            thread = threading.Thread(target=runner)
            thread.start()
            thread.join()

            if "error" in error_container:
                raise error_container["error"]
            return result_container.get("result", self.FALLBACK_RESPONSE)

        try:
            asyncio.get_running_loop()
            # –ï—Å–ª–∏ –µ—Å—Ç—å running loop, –≤—ã–ø–æ–ª–Ω—è–µ–º –∫–æ—Ä—É—Ç–∏–Ω—É –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            return _run_in_new_thread(self.generate_reply(user_message, history, user_id))
        except RuntimeError:
            # –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ loop ‚Äî –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            return asyncio.run(self.generate_reply(user_message, history, user_id))



def generate_ai_reply(user_id: str, user_message: str, history: List[Dict[str, str]] = None) -> str:
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ AI –æ—Ç–≤–µ—Ç–∞
    
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
        user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        history: –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
    Returns:
        –û—Ç–≤–µ—Ç AI
    """
    if history is None:
        history = []

    user_id = str(user_id)
    message_text = user_message or ""
    normalized = _normalize_message(message_text)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏—è
    if len(message_text) > MAX_MESSAGE_LENGTH:
        return f"–°–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ –¥–æ {MAX_MESSAGE_LENGTH} —Å–∏–º–≤–æ–ª–æ–≤."

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    rate_state = _RATE_STATE.get(user_id) or RateState()
    _RATE_STATE[user_id] = rate_state

    # –ö—Ä–∏–∑–∏—Å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —Å—Ä–∞–∑—É, –±–µ–∑ LLM
    if _is_crisis_message(message_text):
        rate_state.last_message = message_text
        rate_state.last_response = CRISIS_RESPONSE
        return CRISIS_RESPONSE

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —á–∞—Å—Ç–æ—Ç—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    if RATE_LIMIT_ENABLED:
        now = time.time()
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        if rate_state.last_request_at and (now - rate_state.last_request_at) < MIN_INTERVAL_SECONDS:
            return RATE_LIMIT_MESSAGE

        # –õ–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –º–∏–Ω—É—Ç—É
        while rate_state.window and (now - rate_state.window[0]) > 60:
            rate_state.window.popleft()
        if len(rate_state.window) >= MAX_PER_MINUTE:
            return RATE_LIMIT_MESSAGE

        rate_state.last_request_at = now
        rate_state.window.append(now)

    # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ/—Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    if normalized in SMALL_TALK:
        reply = random.choice(SMALL_TALK_REPLIES)
        rate_state.last_message = message_text
        rate_state.last_response = reply
        return reply

    # –ö—ç—à –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –≤–æ–ø—Ä–æ—Å
    if rate_state.last_message and rate_state.last_message == message_text and rate_state.last_response:
        return rate_state.last_response
    
    ai_service = AIService()
    
    try:
        reply = ai_service.sync_generate_reply(user_message, history, user_id)
        # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ—Ç–≤–µ—Ç–∞
        if reply and reply != AIService.FALLBACK_RESPONSE and reply != RATE_LIMIT_MESSAGE:
            rate_state.last_message = message_text
            rate_state.last_response = reply
        return reply
    except Exception as e:
        logger.error(f"Error generating AI reply for user {user_id}: {type(e).__name__}")
        return AIService.FALLBACK_RESPONSE
